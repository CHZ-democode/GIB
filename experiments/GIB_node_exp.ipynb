{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Script for experiments with standard learning with GNNs (including GIB-GAT, GAT, GCN and other baselines.)\"\"\"\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "from GIB.experiments.GIB_node_model import GNN, load_model_dict_GNN, get_data, train, test_model, train_baseline\n",
    "from GIB.pytorch_net.util import plot_matrices, to_np_array, Beta_Function, record_data, str2bool, make_dir, eval_tuple, to_string, filter_filename\n",
    "from GIB.util import sample_lognormal, add_distant_neighbors, uniform_prior, process_data_for_nettack, GIB_PATH\n",
    "from GIB.DeepRobust.deeprobust.graph.defense import GCNJaccard\n",
    "from GIB.DeepRobust.deeprobust.graph.defense import RGCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Typical GIB-Cat setting: model_type=\"GAT\", beta1=0.001, beta2=0.1, struct_dropout_mode=(\"Nsampling\",'multi-categorical-sum',0.1,3) (or (\"DNsampling\",'multi-categorical-sum',0.1,3,2));\n",
    "Typical GIB-Bern setting:model_type=\"GAT\", beta1=0.001, beta2=0.1, struct_dropout_mode=(\"Nsampling\",'Bernoulli',0.1,0.5,\"norm\") (or (\"DNsampling\",'Bernoulli',0.1,0.5,\"norm\",2));\n",
    "Standard GAT setting:    model_type=\"GAT\", beta1=-1,    beta2=-1,  struct_dropout_mode=(\"standard\",0.6);\n",
    "Standard GCN setting:    model_type=\"GCN\", beta1=-1,    beta2=-1\n",
    "RGCN setting:            model_type=\"RGCN\"\n",
    "GCNJaccard setting:      model_type=\"GCNJaccard\"\n",
    "\"\"\"\n",
    "exp_id = \"exp1.0\"           # Experiment id, used for the directory name saving the experiment result files.\n",
    "data_type = 'Cora'          # Data type. Choose from \"Cora\", \"Pubmed\", \"citeseer\"\n",
    "model_type = 'GAT'          # Name of the base model. Choose from \"GAT\", \"GCN\", 'GCNJaccard', 'RGCN'. \n",
    "                            # For GIB-Cat and GIB-Bern, still choose model_type=\"GAT\", but set either beta1 or beta2 nonzero.\n",
    "beta1 = 0.001               # coefficient for the XIB term. If -1, this term will turn off.\n",
    "beta2 = 0.1                 # coefficient for the AIB term. If -1, this term will have 0 coefficent (but may still perform sampling, depending on \"struct_dropout_mode\")\n",
    "struct_dropout_mode = (\"Nsampling\", 'multi-categorical-sum', 0.1, 3)  # Mode for how the structural representation is generated. \n",
    "                            # For GIB-Cat, choose from (\"Nsampling\", 'multi-categorical-sum', 0.1, 3) (here 0.1 is temperature, k=3 is the number of sampled neighboring edges with replacement),\n",
    "                            #    and (\"DNsampling\", 'multi-categorical-sum', 0.1, 3, 2) (similar as above, but with 2-hop neighbors)\n",
    "                            # For GIB-Bern, choose from (\"Nsampling\",'Bernoulli',0.1,0.5,\"norm\") (here 0.1 is temperature, 0.5 is the prior for the Bernoulli probability)\n",
    "                            #    and (\"DNsampling\",'Bernoulli',0.1,0.5,\"norm\",2) (with 2-hop neighbors)\n",
    "                            # For standard GAT, choose from (\"standard\", 0.6) (where standard dropout used on the attention weights in GAT)\n",
    "                            #    and (\"standard\", 0.6, 2) (with 2-hop neighbors)\n",
    "train_fraction = 1.         # Fraction of training labels preserved for the training set. Default 1, meaning using the full training set in the standard split.\n",
    "added_edge_fraction = 0.    # Fraction of added random edges. Default 0.\n",
    "feature_noise_ratio = 0.    # Noise ratio for the additive independent Gaussian noise on the features.\n",
    "latent_size = 16            # Latent dimension for GCN-based or GAT-based models.\n",
    "sample_size = 1             # How many Z sampled from each node X.\n",
    "num_layers = 2              # Number of layers for the GNN.\n",
    "reparam_mode = \"diag\"       # Reparameterization mode for XIB. Choose from \"None\", \"diag\" or \"full\"\n",
    "prior_mode = \"mixGau-100\"   # Prior mode. Choose from \"Gaussian\" or \"mixGau-100\" (mixture of 100 Gaussian components)\n",
    "is_anneal_beta = True       # Whether to anneal beta1 and beta2 from 0 up during training. Default True.\n",
    "val_use_mean = True         # Whether during evaluation use the parameter value instead of sampling. If True, during evaluation,\n",
    "                            # XIB will use mean for prediction, and AIB will use the parameter of the categorical distribution for prediction.\n",
    "reparam_all_layers = (-2,)  # Which layers to use XIB, e.g. (1,2,4). Default (-2,), meaning the second last layer. If True, use XIB for all layers.\n",
    "epochs = 2000               # Number of epochs. Default 2000\n",
    "lr = -1                     # Learning rate. If -1, use default learning rate for each model\n",
    "weight_decay = -1           # weight decay. If -1, use default weight decay for each model\n",
    "date_time = \"{0}-{1}\".format(datetime.datetime.now().month, datetime.datetime.now().day)  # Today's month and day. Used for the directory name saving the experiment result files.\n",
    "seed = 0                    # Random seed.\n",
    "idx = \"0\"                   # idx to differentiate different files. Only used if wanting to run the same setting for many times.\n",
    "save_best_model = True      # Whether to save the model with the best validation accuracy.\n",
    "skip_previous = False       # If True, will skip the training if the same setting has already been trained.\n",
    "is_cuda = \"cuda:0\"          # CUDA device. Choose from False, or \"cuda:${NUMBER}\", where the ${NUMBER} is the GPU id.\n",
    "\n",
    "threshold = 0.05            # threshold for GCNJaccard.\n",
    "gamma = 0.5                 # gamma for RGCN\n",
    "\n",
    "try:\n",
    "    # If the current envionrment is Jupyter notebook:\n",
    "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "    import matplotlib.pylab as plt\n",
    "    isplot = True\n",
    "except:\n",
    "    # If the current envionrment is terminal, pass in settings from the command line:\n",
    "    import matplotlib\n",
    "    isplot = True\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--exp_id', default=exp_id, help='experiment ID')\n",
    "    parser.add_argument('--data_type', help='Data type: choose from PROTEINS.', required=True)\n",
    "    parser.add_argument('--model_type', default=\"GAT\", help='Model type: GCN or GAT or GCNJaccard or RGCN')\n",
    "    parser.add_argument('--train_fraction', type=float, default=1., help='train_fraction')\n",
    "    parser.add_argument('--added_edge_fraction', type=float, default=0., help='Fraction of added edges.')\n",
    "    parser.add_argument('--feature_noise_ratio', type=float, default=0., help='Relative amplitude of feature Gaussian noise')\n",
    "    parser.add_argument('--beta1', type=float, default=0.001, help='beta1 value for feature IB, set a float value >= 0.')\n",
    "    parser.add_argument('--beta2', type=float, default=0.1, help='beta2 value for structure IB, set a float value >= 0.')\n",
    "    parser.add_argument('--latent_size', type=int, default=16, help='latent_size')\n",
    "    parser.add_argument('--sample_size', type=int, default=1, help='sample_size')\n",
    "    parser.add_argument('--num_layers', type=int, default=2, help='num_layers')\n",
    "    parser.add_argument('--reparam_mode', default=\"diag\", help='diag, diagg, or full')\n",
    "    parser.add_argument('--prior_mode', default=\"mixGau-100\", help='prior mode for VIB')\n",
    "    parser.add_argument('--struct_dropout_mode', default=\"(standard, 0.6)\", help='mode for structure dropout.')\n",
    "    '''\n",
    "    'Nsampling, categorical/subset/multi-categorical-sum/multi-categorical-max, temperature, sample-neighbor-size'\n",
    "    'Nsampling, Bernoulli, temperature, prior (0~1)', 'norm'/'none'\n",
    "    'DNsampling, categorical/subset/multi-categorical-sum/multi-categorical-max, temperature, sample-neighbor-size, hops'\n",
    "    'DNsampling, Bernoulli, temperature, prior (0~1), 'norm'/'none', hops'\n",
    "    'standard, 0.6'\n",
    "    'standard, 0.6, hops'\n",
    "    '''\n",
    "    parser.add_argument('--is_anneal_beta', type=str2bool, nargs='?', const=True, default=True, help='Whether to anneal beta.')\n",
    "    parser.add_argument('--val_use_mean', type=str2bool, nargs='?', const=True, default=True, help='Whether to use mean of Z during validation.')\n",
    "    parser.add_argument('--reparam_all_layers', type=str, default=\"\\(-2,\\)\", help='Whether to reparameterize all layers.')\n",
    "    parser.add_argument('--epochs', type=int, default=2000, help=\"Number of epochs.\")\n",
    "    parser.add_argument('--lr', type=float, default=-1, help=\"Learning rate.\")\n",
    "    parser.add_argument('--weight_decay', type=float, default=-1, help=\"weight_decay.\")\n",
    "    parser.add_argument('--threshold', type=float, default=0.05, help='threshold for GCNJaccard')\n",
    "    parser.add_argument('--gamma', type=float, default=0.3, help='gamma for RGCN')\n",
    "    parser.add_argument('--save_best_model', type=str2bool, nargs='?', const=True, default=True, help='Whether to save the best model.')\n",
    "    parser.add_argument('--skip_previous', type=str2bool, nargs='?', const=True, default=False, help='Whether to skip previously trained model in the same directory.')\n",
    "    parser.add_argument('--date_time', default=date_time, help=\"Current date and time.\")\n",
    "    parser.add_argument('--seed', type=int, default=0, help='seed')\n",
    "    parser.add_argument('--gpuid', help='an integer for the accumulator', required=True)\n",
    "    parser.add_argument('--idx', default=\"0\", help='idx')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "\n",
    "if \"args\" in locals():\n",
    "    exp_id = args.exp_id\n",
    "    data_type = args.data_type\n",
    "    model_type = args.model_type\n",
    "    train_fraction = args.train_fraction\n",
    "    added_edge_fraction = args.added_edge_fraction\n",
    "    feature_noise_ratio = args.feature_noise_ratio\n",
    "    beta1 = args.beta1\n",
    "    beta2 = args.beta2\n",
    "    latent_size = args.latent_size\n",
    "    sample_size = args.sample_size\n",
    "    num_layers = args.num_layers\n",
    "    reparam_mode = args.reparam_mode\n",
    "    prior_mode = args.prior_mode\n",
    "    struct_dropout_mode = eval_tuple(args.struct_dropout_mode)\n",
    "    is_anneal_beta = args.is_anneal_beta\n",
    "    val_use_mean = args.val_use_mean\n",
    "    reparam_all_layers = eval_tuple(args.reparam_all_layers)\n",
    "    epochs = args.epochs\n",
    "    lr = args.lr\n",
    "    weight_decay = args.weight_decay\n",
    "    threshold = args.threshold\n",
    "    gamma = args.gamma\n",
    "    save_best_model = args.save_best_model\n",
    "    skip_previous = args.skip_previous\n",
    "    date_time = args.date_time\n",
    "    seed = args.seed\n",
    "    idx = args.idx\n",
    "    is_cuda = eval(args.gpuid)\n",
    "    if not isinstance(is_cuda, bool):\n",
    "        is_cuda = \"cuda:{}\".format(is_cuda)\n",
    "\n",
    "baseline = model_type in ['GCNJaccard', 'RGCN']\n",
    "device = torch.device(is_cuda if isinstance(is_cuda, str) else \"cuda\" if is_cuda else \"cpu\")\n",
    "# Directory and filename:\n",
    "dirname = GIB_PATH + \"/{0}_{1}/\".format(exp_id, date_time)\n",
    "if baseline:\n",
    "    filename = dirname + \"{0}_{1}_tr_{2}_ed_{3}_{4}_beta_{5}_{6}_lat_{7}_samp_{8}_lay_{9}_anl_{10}_mean_{11}_reall_{12}_epochs_{13}_lr_{14}_l2_{15}_seed_{16}_threshold_{17}_gamma_{18}_{19}_id_{20}\".format(\n",
    "        data_type, model_type, train_fraction, added_edge_fraction, feature_noise_ratio, beta1, beta2, latent_size, sample_size, num_layers,\n",
    "        is_anneal_beta, val_use_mean, to_string(reparam_all_layers, \"-\"), epochs, lr, weight_decay, seed, threshold, gamma, is_cuda, idx\n",
    "    )\n",
    "else:\n",
    "    filename = dirname + \"{0}_{1}_tr_{2}_ed_{3}_{4}_beta_{5}_{6}_lat_{7}_samp_{8}_lay_{9}_reparam_{10}_prior_{11}_sdrop_{12}_anl_{13}_mean_{14}_reall_{15}_epochs_{16}_lr_{17}_l2_{18}_seed_{19}_{20}_id_{21}\".format(\n",
    "        data_type, model_type, train_fraction, added_edge_fraction, feature_noise_ratio, beta1, beta2, latent_size, sample_size, num_layers, reparam_mode, prior_mode,\n",
    "        to_string(struct_dropout_mode, \"-\"), is_anneal_beta, val_use_mean, to_string(reparam_all_layers, \"-\"), epochs, lr, weight_decay, seed, is_cuda, idx,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seed:\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Setting default hyperparameters:\n",
    "if struct_dropout_mode[0] is None:\n",
    "    struct_dropout_mode = (\"None\",)\n",
    "if lr == -1:\n",
    "    lr = None\n",
    "if weight_decay == -1:\n",
    "    weight_decay = None\n",
    "if beta1 == -1:\n",
    "    beta1 = None\n",
    "if beta1 is None:\n",
    "    beta1_list, reparam_mode, prior_mode = None, None, None\n",
    "else:\n",
    "    if is_anneal_beta:\n",
    "        beta_init = 0\n",
    "        init_length = int(epochs / 4)\n",
    "        anneal_length = int(epochs / 4)\n",
    "        beta_inter = Beta_Function(np.linspace(0,1,anneal_length),1,4)\n",
    "        beta1_inter = beta_inter / 4 * (beta_init - beta1) + beta1\n",
    "        beta1_list = np.concatenate([np.ones(init_length) * beta_init, beta1_inter, \n",
    "                                     np.ones(epochs - init_length - anneal_length + 1) * beta1])\n",
    "    else:\n",
    "        beta1_list = np.ones(epochs + 1) * beta1\n",
    "if beta2 == -1:\n",
    "    beta2_list = None\n",
    "else:\n",
    "    if is_anneal_beta:\n",
    "        beta_init = 0\n",
    "        init_length = int(epochs / 4)\n",
    "        anneal_length = int(epochs / 4)\n",
    "        beta_inter = Beta_Function(np.linspace(0,1,anneal_length),1,4)\n",
    "        beta2_inter = beta_inter / 4 * (beta_init - beta2) + beta2\n",
    "        beta2_list = np.concatenate([np.ones(init_length) * beta_init, beta2_inter, \n",
    "                                     np.ones(epochs - init_length - anneal_length + 1) * beta2])\n",
    "    else:\n",
    "        beta2_list = np.ones(epochs + 1) * beta2\n",
    "\n",
    "# Get Dataset:\n",
    "data, info = get_data(data_type,\n",
    "                      train_fraction=train_fraction,\n",
    "                      added_edge_fraction=added_edge_fraction,\n",
    "                      feature_noise_ratio=feature_noise_ratio,\n",
    "                      seed=seed,\n",
    "                     )\n",
    "\n",
    "if struct_dropout_mode[0] == 'DNsampling' or (struct_dropout_mode[0] == 'standard' and len(struct_dropout_mode) == 3):\n",
    "    add_distant_neighbors(data, struct_dropout_mode[-1])\n",
    "data = process_data_for_nettack(data).to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "\n",
    "if model_type == 'GCNJaccard':\n",
    "    model = GCNJaccard(nfeat=data.features.shape[1], nclass=data.labels.max()+1,\n",
    "                       num_layers=num_layers,\n",
    "                       nhid=latent_size, device=device,\n",
    "                       weight_decay=weight_decay if weight_decay is not None else 5e-4,\n",
    "                       lr=lr if lr is not None else 0.01,\n",
    "                      )\n",
    "elif model_type == 'RGCN':\n",
    "    model = RGCN(nnodes=data.adj.shape[0], nfeat=data.features.shape[1], nclass=data.labels.max()+1,\n",
    "                 num_layers=num_layers,\n",
    "                 nhid=latent_size, device=device,\n",
    "                 lr=lr if lr is not None else 0.01,\n",
    "                 gamma=gamma if gamma is not None else 0.5,\n",
    "                 beta1=beta1 if beta1 is not None else 5e-4,\n",
    "                 beta2=weight_decay if weight_decay is not None else 5e-4,\n",
    "                )\n",
    "else:\n",
    "    # For GIB-GAT, GAT or GCN:\n",
    "    model = GNN(\n",
    "        model_type=model_type,\n",
    "        num_features=info[\"num_features\"],\n",
    "        num_classes=info[\"num_classes\"],\n",
    "        normalize=True,\n",
    "        reparam_mode=reparam_mode,\n",
    "        prior_mode=prior_mode,\n",
    "        latent_size=latent_size,\n",
    "        sample_size=sample_size,\n",
    "        num_layers=num_layers,\n",
    "        struct_dropout_mode=struct_dropout_mode,\n",
    "        dropout=True,\n",
    "        val_use_mean=val_use_mean,\n",
    "        reparam_all_layers=reparam_all_layers,\n",
    "        is_cuda=is_cuda,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filename + \"\\n\")\n",
    "\n",
    "if skip_previous:\n",
    "    filename_core = \"_\".join(filename.split(\"/\")[-1].split(\"_\")[:-3])\n",
    "    if filename_core.endswith(\"cuda\"):\n",
    "        filename_core = filename_core[:-5]\n",
    "    cand_filename = filter_filename(dirname, include=filename_core)\n",
    "    if len(cand_filename) == 0:\n",
    "        skip_previous = False\n",
    "if skip_previous:\n",
    "    print(\"File already exists at {} with {}\".format(dirname, cand_filename))\n",
    "else:\n",
    "    if baseline:\n",
    "        data_record = train_baseline(model, model_type, data, device, threshold, filename, epochs, save_best_model=save_best_model, verbose=True)\n",
    "    else:\n",
    "        data_record = train(\n",
    "            model=model,\n",
    "            data=data,\n",
    "            data_type=data_type,\n",
    "            model_type=model_type,\n",
    "            loss_type=info['loss'],\n",
    "            beta1_list=beta1_list,\n",
    "            beta2_list=beta2_list,\n",
    "            epochs=epochs,\n",
    "            inspect_interval=200 if isplot else 20,\n",
    "            verbose=True,\n",
    "            isplot=isplot,\n",
    "            filename=filename,\n",
    "            compute_metrics=None,\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            save_best_model=save_best_model,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
